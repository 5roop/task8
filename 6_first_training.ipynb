{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# import torch\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "# cuda.select_device(0)\n",
    "# import os\n",
    "# os.system(\"bash /home/peterr/clean_tmp_script.sh\")\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "f = \"transfer_10.csv\"\n",
    "data_dir = \"/home/peterr/macocu/task8/transfer_10/\"\n",
    "df = pd.read_csv(f)\n",
    "modelname = \"300_\"\n",
    "df[\"path\"] = data_dir + df[\"hashname\"]\n",
    "df[\"sentence\"] = df.human_transcript\n",
    "df = df.loc[:1000, [\"path\", \"sentence\"]]\n",
    "\n",
    "# Use old or new vocab?\n",
    "os.system(\"cp vocab_old.json vocab.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "common_voice_train_df, common_voice_test_df = train_test_split(df, test_size=500, random_state=42)\n",
    "\n",
    "\n",
    "common_voice_train_df.reset_index(drop=True, inplace=True)\n",
    "common_voice_test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Audio\n",
    "def load_audio(path):\n",
    "    return datasets.Audio(sampling_rate=16000).decode_example(path)\n",
    "\n",
    "# Adding audio\n",
    "common_voice_train_df.loc[:, \"audio\"] = common_voice_train_df.path.apply(load_audio)\n",
    "common_voice_test_df.loc[:, \"audio\"] = common_voice_test_df.path.apply(load_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initiating a dataset from pandas dataframe\n",
    "common_voice_train_dataset = datasets.Dataset.from_pandas(common_voice_train_df)\n",
    "common_voice_test_dataset = datasets.Dataset.from_pandas(common_voice_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/home/peterr/macocu/task8/transfer_10/5T4HXO1WsDU_2830.21-2850.13.wav',\n",
       " 'sentence': 'treba stvoriti pretpostavke da imao novac koji ovdje dajemo onima koji inače primaju najveće mirovine i najveće plaće bez potrebe i bez da nas je to itko pitao i suprotno našim predizbornim obećanjima svih stranaka ovdje se ovo radi dakle potpuno odstupamo',\n",
       " 'audio': {'array': [-0.017242431640625,\n",
       "   -0.01251220703125,\n",
       "   0.00054931640625,\n",
       "   0.01605224609375,\n",
       "   0.02593994140625,\n",
       "   0.026397705078125,\n",
       "   0.02056884765625,\n",
       "   0.01837158203125,\n",
       "   0.020477294921875,\n",
       "   0.016754150390625,\n",
       "   0.01123046875,\n",
       "   0.0126953125,\n",
       "   0.01422119140625,\n",
       "   0.01513671875,\n",
       "   0.018463134765625,\n",
       "   0.02191162109375,\n",
       "   0.02496337890625,\n",
       "   0.02276611328125,\n",
       "   0.016143798828125,\n",
       "   0.0028076171875,\n",
       "   -0.0074462890625,\n",
       "   -0.010955810546875,\n",
       "   -0.00982666015625,\n",
       "   -0.007476806640625,\n",
       "   -0.005157470703125,\n",
       "   -9.1552734375e-05,\n",
       "   0.0013427734375,\n",
       "   -0.004150390625,\n",
       "   -0.007598876953125,\n",
       "   -0.0076904296875,\n",
       "   -0.008514404296875,\n",
       "   -0.009185791015625,\n",
       "   -0.0107421875,\n",
       "   -0.011199951171875,\n",
       "   -0.00640869140625,\n",
       "   0.001068115234375,\n",
       "   0.0087890625,\n",
       "   0.014404296875,\n",
       "   0.013275146484375,\n",
       "   0.005401611328125,\n",
       "   -0.00225830078125,\n",
       "   -0.007476806640625,\n",
       "   -0.00946044921875,\n",
       "   -0.007110595703125,\n",
       "   -0.00421142578125,\n",
       "   -0.002899169921875,\n",
       "   0.001312255859375,\n",
       "   0.00970458984375,\n",
       "   0.01483154296875,\n",
       "   0.01416015625,\n",
       "   0.011474609375,\n",
       "   0.008544921875,\n",
       "   0.004791259765625,\n",
       "   0.00048828125,\n",
       "   -0.00067138671875,\n",
       "   0.003387451171875,\n",
       "   0.0079345703125,\n",
       "   0.00927734375,\n",
       "   0.010101318359375,\n",
       "   0.008575439453125,\n",
       "   0.00543212890625,\n",
       "   0.003387451171875,\n",
       "   -0.004119873046875,\n",
       "   -0.01678466796875,\n",
       "   -0.020263671875,\n",
       "   -0.01593017578125,\n",
       "   -0.013031005859375,\n",
       "   -0.01165771484375,\n",
       "   -0.008087158203125,\n",
       "   -0.004119873046875,\n",
       "   -0.007720947265625,\n",
       "   -0.01434326171875,\n",
       "   -0.017578125,\n",
       "   -0.018585205078125,\n",
       "   -0.01873779296875,\n",
       "   -0.016845703125,\n",
       "   -0.00946044921875,\n",
       "   -0.000213623046875,\n",
       "   0.007232666015625,\n",
       "   0.0135498046875,\n",
       "   0.017791748046875,\n",
       "   0.01422119140625,\n",
       "   0.011627197265625,\n",
       "   0.01470947265625,\n",
       "   0.01220703125,\n",
       "   0.0113525390625,\n",
       "   0.01824951171875,\n",
       "   0.024444580078125,\n",
       "   0.026702880859375,\n",
       "   0.02685546875,\n",
       "   0.02862548828125,\n",
       "   0.027496337890625,\n",
       "   0.02459716796875,\n",
       "   0.020263671875,\n",
       "   0.012481689453125,\n",
       "   0.00323486328125,\n",
       "   -0.00537109375,\n",
       "   -0.011566162109375,\n",
       "   -0.014312744140625,\n",
       "   -0.016265869140625,\n",
       "   -0.01953125,\n",
       "   -0.023834228515625,\n",
       "   -0.02764892578125,\n",
       "   -0.03265380859375,\n",
       "   -0.0384521484375,\n",
       "   -0.037689208984375,\n",
       "   -0.03680419921875,\n",
       "   -0.03448486328125,\n",
       "   -0.026397705078125,\n",
       "   -0.019012451171875,\n",
       "   -0.01239013671875,\n",
       "   -0.006866455078125,\n",
       "   -0.004974365234375,\n",
       "   -0.00225830078125,\n",
       "   0.004852294921875,\n",
       "   0.0140380859375,\n",
       "   0.020599365234375,\n",
       "   0.021942138671875,\n",
       "   0.023406982421875,\n",
       "   0.02764892578125,\n",
       "   0.03399658203125,\n",
       "   0.0360107421875,\n",
       "   0.038909912109375,\n",
       "   0.04449462890625,\n",
       "   0.04461669921875,\n",
       "   0.038116455078125,\n",
       "   0.029632568359375,\n",
       "   0.02392578125,\n",
       "   0.014404296875,\n",
       "   0.0020751953125,\n",
       "   -0.006378173828125,\n",
       "   -0.007720947265625,\n",
       "   -0.00634765625,\n",
       "   -0.0091552734375,\n",
       "   -0.01519775390625,\n",
       "   -0.023529052734375,\n",
       "   -0.02899169921875,\n",
       "   -0.031524658203125,\n",
       "   -0.03436279296875,\n",
       "   -0.037567138671875,\n",
       "   -0.038055419921875,\n",
       "   -0.0374755859375,\n",
       "   -0.037689208984375,\n",
       "   -0.0345458984375,\n",
       "   -0.03106689453125,\n",
       "   -0.0277099609375,\n",
       "   -0.022125244140625,\n",
       "   -0.020416259765625,\n",
       "   -0.021453857421875,\n",
       "   -0.02020263671875,\n",
       "   -0.014556884765625,\n",
       "   -0.00921630859375,\n",
       "   -0.005645751953125,\n",
       "   0.001556396484375,\n",
       "   0.0076904296875,\n",
       "   0.0120849609375,\n",
       "   0.0184326171875,\n",
       "   0.0235595703125,\n",
       "   0.02490234375,\n",
       "   0.025115966796875,\n",
       "   0.0277099609375,\n",
       "   0.026031494140625,\n",
       "   0.021270751953125,\n",
       "   0.022369384765625,\n",
       "   0.025146484375,\n",
       "   0.026397705078125,\n",
       "   0.02667236328125,\n",
       "   0.025390625,\n",
       "   0.02154541015625,\n",
       "   0.01873779296875,\n",
       "   0.015625,\n",
       "   0.00982666015625,\n",
       "   0.000640869140625,\n",
       "   -0.00262451171875,\n",
       "   -0.0025634765625,\n",
       "   -0.006439208984375,\n",
       "   -0.0079345703125,\n",
       "   -0.0054931640625,\n",
       "   -0.0059814453125,\n",
       "   -0.007965087890625,\n",
       "   -0.00860595703125,\n",
       "   -0.00823974609375,\n",
       "   -0.0069580078125,\n",
       "   -0.005035400390625,\n",
       "   -0.0040283203125,\n",
       "   -0.001983642578125,\n",
       "   -0.001739501953125,\n",
       "   0.00140380859375,\n",
       "   0.00604248046875,\n",
       "   0.005126953125,\n",
       "   0.00244140625,\n",
       "   0.003662109375,\n",
       "   0.003204345703125,\n",
       "   0.001190185546875,\n",
       "   -0.0010986328125,\n",
       "   -0.001373291015625,\n",
       "   0.00445556640625,\n",
       "   0.009368896484375,\n",
       "   0.00958251953125,\n",
       "   0.007598876953125,\n",
       "   0.00360107421875,\n",
       "   0.001983642578125,\n",
       "   -0.002410888671875,\n",
       "   -0.00665283203125,\n",
       "   -0.005035400390625,\n",
       "   -0.002105712890625,\n",
       "   -0.00140380859375,\n",
       "   0.000823974609375,\n",
       "   0.004638671875,\n",
       "   0.0035400390625,\n",
       "   0.001495361328125,\n",
       "   -0.006805419921875,\n",
       "   -0.019775390625,\n",
       "   -0.026458740234375,\n",
       "   -0.025787353515625,\n",
       "   -0.02178955078125,\n",
       "   -0.015594482421875,\n",
       "   -0.013641357421875,\n",
       "   -0.01519775390625,\n",
       "   -0.016082763671875,\n",
       "   -0.01470947265625,\n",
       "   -0.010772705078125,\n",
       "   -0.009063720703125,\n",
       "   -0.01361083984375,\n",
       "   -0.015625,\n",
       "   -0.01605224609375,\n",
       "   -0.017852783203125,\n",
       "   -0.009368896484375,\n",
       "   0.005126953125,\n",
       "   0.015777587890625,\n",
       "   0.017486572265625,\n",
       "   0.01458740234375,\n",
       "   0.014373779296875,\n",
       "   0.0140380859375,\n",
       "   0.01263427734375,\n",
       "   0.012664794921875,\n",
       "   0.011993408203125,\n",
       "   0.009307861328125,\n",
       "   0.0111083984375,\n",
       "   0.017059326171875,\n",
       "   0.021514892578125,\n",
       "   0.024261474609375,\n",
       "   0.023345947265625,\n",
       "   0.01666259765625,\n",
       "   0.010223388671875,\n",
       "   0.005828857421875,\n",
       "   0.0028076171875,\n",
       "   0.001312255859375,\n",
       "   0.0001220703125,\n",
       "   -0.00152587890625,\n",
       "   -0.0018310546875,\n",
       "   -0.001617431640625,\n",
       "   0.000396728515625,\n",
       "   -0.001739501953125,\n",
       "   -0.010833740234375,\n",
       "   -0.01641845703125,\n",
       "   -0.018798828125,\n",
       "   -0.021514892578125,\n",
       "   -0.023345947265625,\n",
       "   -0.020477294921875,\n",
       "   -0.016357421875,\n",
       "   -0.013702392578125,\n",
       "   -0.012176513671875,\n",
       "   -0.012115478515625,\n",
       "   -0.010223388671875,\n",
       "   -0.0059814453125,\n",
       "   -0.002655029296875,\n",
       "   -0.002471923828125,\n",
       "   6.103515625e-05,\n",
       "   0.0040283203125,\n",
       "   0.005615234375,\n",
       "   0.004730224609375,\n",
       "   0.0052490234375,\n",
       "   0.009307861328125,\n",
       "   0.0130615234375,\n",
       "   0.01519775390625,\n",
       "   0.01580810546875,\n",
       "   0.012969970703125,\n",
       "   0.009246826171875,\n",
       "   0.006927490234375,\n",
       "   0.002044677734375,\n",
       "   0.000823974609375,\n",
       "   0.005645751953125,\n",
       "   0.007965087890625,\n",
       "   0.006378173828125,\n",
       "   0.005218505859375,\n",
       "   0.00653076171875,\n",
       "   0.00897216796875,\n",
       "   0.01287841796875,\n",
       "   0.014617919921875,\n",
       "   0.01226806640625,\n",
       "   0.0087890625,\n",
       "   0.007232666015625,\n",
       "   0.006744384765625,\n",
       "   0.008056640625,\n",
       "   0.008880615234375,\n",
       "   0.00592041015625,\n",
       "   0.000885009765625,\n",
       "   -0.004180908203125,\n",
       "   -0.00823974609375,\n",
       "   -0.00958251953125,\n",
       "   -0.01031494140625,\n",
       "   -0.01483154296875,\n",
       "   -0.01898193359375,\n",
       "   -0.02685546875,\n",
       "   -0.029327392578125,\n",
       "   -0.0225830078125,\n",
       "   -0.02044677734375,\n",
       "   -0.02105712890625,\n",
       "   -0.0206298828125,\n",
       "   -0.019744873046875,\n",
       "   -0.0177001953125,\n",
       "   -0.01446533203125,\n",
       "   -0.012542724609375,\n",
       "   -0.008819580078125,\n",
       "   -0.0025634765625,\n",
       "   0.001861572265625,\n",
       "   0.007598876953125,\n",
       "   0.01251220703125,\n",
       "   0.015777587890625,\n",
       "   0.019073486328125,\n",
       "   0.01885986328125,\n",
       "   0.019775390625,\n",
       "   0.02386474609375,\n",
       "   0.02947998046875,\n",
       "   0.0308837890625,\n",
       "   0.026153564453125,\n",
       "   0.021759033203125,\n",
       "   0.0181884765625,\n",
       "   0.017822265625,\n",
       "   0.019073486328125,\n",
       "   0.017578125,\n",
       "   0.015045166015625,\n",
       "   0.01641845703125,\n",
       "   0.020294189453125,\n",
       "   0.017303466796875,\n",
       "   0.01031494140625,\n",
       "   0.005279541015625,\n",
       "   0.001434326171875,\n",
       "   -0.000579833984375,\n",
       "   0.00140380859375,\n",
       "   0.00018310546875,\n",
       "   -0.00579833984375,\n",
       "   -0.0079345703125,\n",
       "   -0.007415771484375,\n",
       "   -0.009307861328125,\n",
       "   -0.01116943359375,\n",
       "   -0.009918212890625,\n",
       "   -0.0076904296875,\n",
       "   -0.006561279296875,\n",
       "   -0.00823974609375,\n",
       "   -0.007904052734375,\n",
       "   -0.002105712890625,\n",
       "   0.00128173828125,\n",
       "   -0.0030517578125,\n",
       "   -0.0047607421875,\n",
       "   -0.00439453125,\n",
       "   -0.00482177734375,\n",
       "   -0.003204345703125,\n",
       "   -0.002593994140625,\n",
       "   -0.004150390625,\n",
       "   -0.007720947265625,\n",
       "   -0.0079345703125,\n",
       "   -0.0062255859375,\n",
       "   -0.007232666015625,\n",
       "   -0.01263427734375,\n",
       "   -0.018829345703125,\n",
       "   -0.01885986328125,\n",
       "   -0.019134521484375,\n",
       "   -0.021820068359375,\n",
       "   -0.01904296875,\n",
       "   -0.014129638671875,\n",
       "   -0.01214599609375,\n",
       "   -0.01202392578125,\n",
       "   -0.01055908203125,\n",
       "   -0.009185791015625,\n",
       "   -0.005096435546875,\n",
       "   0.000701904296875,\n",
       "   0.002685546875,\n",
       "   0.002471923828125,\n",
       "   0.00335693359375,\n",
       "   0.008148193359375,\n",
       "   0.01556396484375,\n",
       "   0.021514892578125,\n",
       "   0.02099609375,\n",
       "   0.018402099609375,\n",
       "   0.018157958984375,\n",
       "   0.017059326171875,\n",
       "   0.01513671875,\n",
       "   0.01605224609375,\n",
       "   0.018768310546875,\n",
       "   0.01873779296875,\n",
       "   0.0159912109375,\n",
       "   0.01104736328125,\n",
       "   0.00738525390625,\n",
       "   0.00457763671875,\n",
       "   0.0018310546875,\n",
       "   -0.001007080078125,\n",
       "   -0.0062255859375,\n",
       "   -0.0128173828125,\n",
       "   -0.012359619140625,\n",
       "   -0.013671875,\n",
       "   -0.018463134765625,\n",
       "   -0.020782470703125,\n",
       "   -0.022918701171875,\n",
       "   -0.022308349609375,\n",
       "   -0.018035888671875,\n",
       "   -0.01849365234375,\n",
       "   -0.02105712890625,\n",
       "   -0.018707275390625,\n",
       "   -0.012176513671875,\n",
       "   -0.00823974609375,\n",
       "   -0.010345458984375,\n",
       "   -0.010650634765625,\n",
       "   -0.00390625,\n",
       "   0.00433349609375,\n",
       "   0.01055908203125,\n",
       "   0.0135498046875,\n",
       "   0.015411376953125,\n",
       "   0.0167236328125,\n",
       "   0.021087646484375,\n",
       "   0.022186279296875,\n",
       "   0.021820068359375,\n",
       "   0.0269775390625,\n",
       "   0.03277587890625,\n",
       "   0.033660888671875,\n",
       "   0.0279541015625,\n",
       "   0.0255126953125,\n",
       "   0.027496337890625,\n",
       "   0.02978515625,\n",
       "   0.026123046875,\n",
       "   0.013641357421875,\n",
       "   0.00439453125,\n",
       "   0.00396728515625,\n",
       "   0.002166748046875,\n",
       "   -0.002838134765625,\n",
       "   -0.002777099609375,\n",
       "   -0.001190185546875,\n",
       "   -0.001983642578125,\n",
       "   -0.0078125,\n",
       "   -0.012542724609375,\n",
       "   -0.013916015625,\n",
       "   -0.01324462890625,\n",
       "   -0.012420654296875,\n",
       "   -0.017608642578125,\n",
       "   -0.0235595703125,\n",
       "   -0.023101806640625,\n",
       "   -0.017669677734375,\n",
       "   -0.01141357421875,\n",
       "   -0.0086669921875,\n",
       "   -0.010711669921875,\n",
       "   -0.01043701171875,\n",
       "   -0.006805419921875,\n",
       "   -0.00372314453125,\n",
       "   -0.003570556640625,\n",
       "   -0.007415771484375,\n",
       "   -0.010650634765625,\n",
       "   -0.012298583984375,\n",
       "   -0.015472412109375,\n",
       "   -0.016693115234375,\n",
       "   -0.013336181640625,\n",
       "   -0.008270263671875,\n",
       "   -0.006500244140625,\n",
       "   -0.0068359375,\n",
       "   -0.007354736328125,\n",
       "   -0.0054931640625,\n",
       "   -0.0023193359375,\n",
       "   -0.003997802734375,\n",
       "   -0.008758544921875,\n",
       "   -0.015716552734375,\n",
       "   -0.017486572265625,\n",
       "   -0.01043701171875,\n",
       "   -0.00213623046875,\n",
       "   0.001190185546875,\n",
       "   0.00372314453125,\n",
       "   0.008087158203125,\n",
       "   0.00628662109375,\n",
       "   0.002960205078125,\n",
       "   0.007171630859375,\n",
       "   0.013397216796875,\n",
       "   0.0169677734375,\n",
       "   0.01702880859375,\n",
       "   0.012969970703125,\n",
       "   0.008941650390625,\n",
       "   0.01214599609375,\n",
       "   0.01837158203125,\n",
       "   0.02557373046875,\n",
       "   0.0272216796875,\n",
       "   0.022735595703125,\n",
       "   0.016815185546875,\n",
       "   0.00982666015625,\n",
       "   0.006744384765625,\n",
       "   0.0076904296875,\n",
       "   0.0064697265625,\n",
       "   0.003814697265625,\n",
       "   0.00146484375,\n",
       "   -0.00341796875,\n",
       "   -0.008544921875,\n",
       "   -0.00836181640625,\n",
       "   -0.004364013671875,\n",
       "   -0.0010986328125,\n",
       "   -0.0018310546875,\n",
       "   -0.0045166015625,\n",
       "   -0.0052490234375,\n",
       "   -0.005767822265625,\n",
       "   -0.002227783203125,\n",
       "   -0.001739501953125,\n",
       "   -0.00860595703125,\n",
       "   -0.01513671875,\n",
       "   -0.02069091796875,\n",
       "   -0.019012451171875,\n",
       "   -0.0128173828125,\n",
       "   -0.007080078125,\n",
       "   -0.00299072265625,\n",
       "   -0.002899169921875,\n",
       "   -0.00836181640625,\n",
       "   -0.0162353515625,\n",
       "   -0.022491455078125,\n",
       "   -0.01959228515625,\n",
       "   -0.010223388671875,\n",
       "   -0.010101318359375,\n",
       "   -0.01568603515625,\n",
       "   -0.0137939453125,\n",
       "   -0.00787353515625,\n",
       "   0.0023193359375,\n",
       "   0.014556884765625,\n",
       "   0.01837158203125,\n",
       "   0.018341064453125,\n",
       "   0.018310546875,\n",
       "   0.0140380859375,\n",
       "   0.012908935546875,\n",
       "   0.01385498046875,\n",
       "   0.016998291015625,\n",
       "   0.01910400390625,\n",
       "   0.01641845703125,\n",
       "   0.019195556640625,\n",
       "   0.02398681640625,\n",
       "   0.02447509765625,\n",
       "   0.025054931640625,\n",
       "   0.0184326171875,\n",
       "   0.00341796875,\n",
       "   -0.004425048828125,\n",
       "   -0.00469970703125,\n",
       "   -0.00653076171875,\n",
       "   -0.008331298828125,\n",
       "   -0.0115966796875,\n",
       "   -0.0167236328125,\n",
       "   -0.021331787109375,\n",
       "   -0.022735595703125,\n",
       "   -0.0228271484375,\n",
       "   -0.0228271484375,\n",
       "   -0.022064208984375,\n",
       "   -0.02362060546875,\n",
       "   -0.02972412109375,\n",
       "   -0.0321044921875,\n",
       "   -0.0267333984375,\n",
       "   -0.020172119140625,\n",
       "   -0.01422119140625,\n",
       "   -0.00726318359375,\n",
       "   -0.00347900390625,\n",
       "   0.00128173828125,\n",
       "   0.006866455078125,\n",
       "   0.00848388671875,\n",
       "   0.0076904296875,\n",
       "   0.01031494140625,\n",
       "   0.012969970703125,\n",
       "   0.01715087890625,\n",
       "   0.022308349609375,\n",
       "   0.027740478515625,\n",
       "   0.029632568359375,\n",
       "   0.027740478515625,\n",
       "   0.02337646484375,\n",
       "   0.01727294921875,\n",
       "   0.01556396484375,\n",
       "   0.014984130859375,\n",
       "   0.011505126953125,\n",
       "   0.00726318359375,\n",
       "   0.001739501953125,\n",
       "   -0.003021240234375,\n",
       "   -0.00604248046875,\n",
       "   -0.005950927734375,\n",
       "   -0.002410888671875,\n",
       "   -0.00262451171875,\n",
       "   -0.00909423828125,\n",
       "   -0.01220703125,\n",
       "   -0.00848388671875,\n",
       "   -0.0050048828125,\n",
       "   -0.00372314453125,\n",
       "   0.00079345703125,\n",
       "   0.001708984375,\n",
       "   0.001434326171875,\n",
       "   0.001800537109375,\n",
       "   0.001739501953125,\n",
       "   0.001434326171875,\n",
       "   0.000885009765625,\n",
       "   -0.00244140625,\n",
       "   -0.0052490234375,\n",
       "   -0.001556396484375,\n",
       "   0.003204345703125,\n",
       "   0.005828857421875,\n",
       "   0.006805419921875,\n",
       "   0.003662109375,\n",
       "   -0.003631591796875,\n",
       "   -0.009552001953125,\n",
       "   -0.01287841796875,\n",
       "   -0.015777587890625,\n",
       "   -0.017730712890625,\n",
       "   -0.019989013671875,\n",
       "   -0.023406982421875,\n",
       "   -0.024505615234375,\n",
       "   -0.02301025390625,\n",
       "   -0.018829345703125,\n",
       "   -0.01177978515625,\n",
       "   -0.006439208984375,\n",
       "   -0.00689697265625,\n",
       "   -0.00946044921875,\n",
       "   -0.01171875,\n",
       "   -0.01593017578125,\n",
       "   -0.016143798828125,\n",
       "   -0.009765625,\n",
       "   -0.004730224609375,\n",
       "   -0.002288818359375,\n",
       "   0.00189208984375,\n",
       "   0.006561279296875,\n",
       "   0.01019287109375,\n",
       "   0.015869140625,\n",
       "   0.020965576171875,\n",
       "   0.02142333984375,\n",
       "   0.020751953125,\n",
       "   0.021759033203125,\n",
       "   0.02484130859375,\n",
       "   0.028656005859375,\n",
       "   0.02960205078125,\n",
       "   0.0279541015625,\n",
       "   0.0274658203125,\n",
       "   0.028594970703125,\n",
       "   0.02972412109375,\n",
       "   0.033660888671875,\n",
       "   0.034912109375,\n",
       "   0.025146484375,\n",
       "   0.012359619140625,\n",
       "   0.003662109375,\n",
       "   -0.0030517578125,\n",
       "   -0.005706787109375,\n",
       "   -0.003143310546875,\n",
       "   -0.003814697265625,\n",
       "   -0.009063720703125,\n",
       "   -0.013427734375,\n",
       "   -0.016998291015625,\n",
       "   -0.01837158203125,\n",
       "   -0.016326904296875,\n",
       "   -0.014678955078125,\n",
       "   -0.015716552734375,\n",
       "   -0.018035888671875,\n",
       "   -0.018829345703125,\n",
       "   -0.01434326171875,\n",
       "   -0.00604248046875,\n",
       "   -0.000946044921875,\n",
       "   -0.000244140625,\n",
       "   -9.1552734375e-05,\n",
       "   0.00164794921875,\n",
       "   0.005157470703125,\n",
       "   0.009246826171875,\n",
       "   0.01031494140625,\n",
       "   0.008331298828125,\n",
       "   0.00543212890625,\n",
       "   0.002655029296875,\n",
       "   0.001556396484375,\n",
       "   0.0020751953125,\n",
       "   0.0025634765625,\n",
       "   0.00128173828125,\n",
       "   -0.00341796875,\n",
       "   -0.010467529296875,\n",
       "   -0.015289306640625,\n",
       "   -0.015350341796875,\n",
       "   -0.0123291015625,\n",
       "   -0.010711669921875,\n",
       "   -0.013031005859375,\n",
       "   -0.0169677734375,\n",
       "   -0.018035888671875,\n",
       "   -0.0142822265625,\n",
       "   -0.00958251953125,\n",
       "   -0.00775146484375,\n",
       "   -0.00726318359375,\n",
       "   -0.004180908203125,\n",
       "   0.00103759765625,\n",
       "   0.005218505859375,\n",
       "   0.009918212890625,\n",
       "   0.016632080078125,\n",
       "   0.020965576171875,\n",
       "   0.019744873046875,\n",
       "   0.01629638671875,\n",
       "   0.013916015625,\n",
       "   0.012725830078125,\n",
       "   0.014007568359375,\n",
       "   0.018157958984375,\n",
       "   0.0185546875,\n",
       "   0.013671875,\n",
       "   0.010711669921875,\n",
       "   0.010467529296875,\n",
       "   0.00616455078125,\n",
       "   -0.0009765625,\n",
       "   -0.0048828125,\n",
       "   -0.006439208984375,\n",
       "   -0.01080322265625,\n",
       "   -0.017486572265625,\n",
       "   -0.019744873046875,\n",
       "   -0.0166015625,\n",
       "   -0.013580322265625,\n",
       "   -0.01251220703125,\n",
       "   -0.011993408203125,\n",
       "   -0.01226806640625,\n",
       "   -0.013519287109375,\n",
       "   -0.01116943359375,\n",
       "   -0.00482177734375,\n",
       "   -0.002838134765625,\n",
       "   -0.005889892578125,\n",
       "   -0.0078125,\n",
       "   -0.00823974609375,\n",
       "   -0.005401611328125,\n",
       "   -0.000640869140625,\n",
       "   0.00518798828125,\n",
       "   0.007537841796875,\n",
       "   0.003570556640625,\n",
       "   -0.000396728515625,\n",
       "   -0.00079345703125,\n",
       "   0.000518798828125,\n",
       "   -0.0001220703125,\n",
       "   -0.001434326171875,\n",
       "   -0.0040283203125,\n",
       "   -0.006256103515625,\n",
       "   -0.005828857421875,\n",
       "   -0.006500244140625,\n",
       "   -0.005279541015625,\n",
       "   -0.0018310546875,\n",
       "   0.004150390625,\n",
       "   0.00518798828125,\n",
       "   0.006561279296875,\n",
       "   0.0108642578125,\n",
       "   0.01507568359375,\n",
       "   0.016082763671875,\n",
       "   0.016754150390625,\n",
       "   0.017974853515625,\n",
       "   0.01666259765625,\n",
       "   0.021575927734375,\n",
       "   0.029571533203125,\n",
       "   0.0352783203125,\n",
       "   0.03363037109375,\n",
       "   0.025726318359375,\n",
       "   0.022308349609375,\n",
       "   0.021728515625,\n",
       "   0.02203369140625,\n",
       "   0.018585205078125,\n",
       "   0.01580810546875,\n",
       "   0.00494384765625,\n",
       "   -0.007476806640625,\n",
       "   -0.012054443359375,\n",
       "   -0.013427734375,\n",
       "   -0.013671875,\n",
       "   -0.0220947265625,\n",
       "   -0.031524658203125,\n",
       "   -0.03948974609375,\n",
       "   -0.043182373046875,\n",
       "   -0.040496826171875,\n",
       "   -0.041107177734375,\n",
       "   -0.04217529296875,\n",
       "   -0.04119873046875,\n",
       "   -0.040985107421875,\n",
       "   -0.03900146484375,\n",
       "   -0.035552978515625,\n",
       "   -0.028228759765625,\n",
       "   -0.024169921875,\n",
       "   -0.023193359375,\n",
       "   -0.0185546875,\n",
       "   -0.020355224609375,\n",
       "   -0.015869140625,\n",
       "   0.00152587890625,\n",
       "   0.01531982421875,\n",
       "   0.0240478515625,\n",
       "   0.029022216796875,\n",
       "   0.02862548828125,\n",
       "   0.02947998046875,\n",
       "   0.028900146484375,\n",
       "   0.034332275390625,\n",
       "   0.036285400390625,\n",
       "   0.036407470703125,\n",
       "   0.034912109375,\n",
       "   0.035186767578125,\n",
       "   0.018341064453125,\n",
       "   0.01898193359375,\n",
       "   0.050048828125,\n",
       "   0.04510498046875,\n",
       "   0.02862548828125,\n",
       "   0.00439453125,\n",
       "   -0.017608642578125,\n",
       "   -0.00384521484375,\n",
       "   -0.00347900390625,\n",
       "   -0.019287109375,\n",
       "   -0.017791748046875,\n",
       "   -0.02020263671875,\n",
       "   -0.024810791015625,\n",
       "   -0.02459716796875,\n",
       "   -0.02392578125,\n",
       "   -0.01800537109375,\n",
       "   -0.02117919921875,\n",
       "   -0.0330810546875,\n",
       "   -0.031494140625,\n",
       "   -0.024749755859375,\n",
       "   -0.01263427734375,\n",
       "   -0.005615234375,\n",
       "   -0.002960205078125,\n",
       "   -0.000335693359375,\n",
       "   0.001068115234375,\n",
       "   0.003173828125,\n",
       "   0.00714111328125,\n",
       "   0.008697509765625,\n",
       "   0.0035400390625,\n",
       "   6.103515625e-05,\n",
       "   -0.00128173828125,\n",
       "   0.001678466796875,\n",
       "   0.009033203125,\n",
       "   0.01239013671875,\n",
       "   0.0078125,\n",
       "   0.004150390625,\n",
       "   -0.01336669921875,\n",
       "   -0.027862548828125,\n",
       "   0.020721435546875,\n",
       "   0.049468994140625,\n",
       "   -0.00042724609375,\n",
       "   -0.064544677734375,\n",
       "   -0.0516357421875,\n",
       "   0.002349853515625,\n",
       "   0.048675537109375,\n",
       "   0.030731201171875,\n",
       "   -0.016632080078125,\n",
       "   -0.006805419921875,\n",
       "   0.019287109375,\n",
       "   0.021820068359375,\n",
       "   0.0255126953125,\n",
       "   -0.003875732421875,\n",
       "   -0.02496337890625,\n",
       "   -0.0281982421875,\n",
       "   -0.000701904296875,\n",
       "   0.023773193359375,\n",
       "   0.010498046875,\n",
       "   -0.009735107421875,\n",
       "   -0.00714111328125,\n",
       "   0.000640869140625,\n",
       "   0.003753662109375,\n",
       "   0.000885009765625,\n",
       "   0.005462646484375,\n",
       "   0.003814697265625,\n",
       "   -0.007171630859375,\n",
       "   -0.0111083984375,\n",
       "   -0.009674072265625,\n",
       "   -0.012908935546875,\n",
       "   -0.0045166015625,\n",
       "   -0.00360107421875,\n",
       "   -0.00421142578125,\n",
       "   -0.025726318359375,\n",
       "   -0.026763916015625,\n",
       "   0.00384521484375,\n",
       "   0.00823974609375,\n",
       "   0.0120849609375,\n",
       "   0.000244140625,\n",
       "   -0.01068115234375,\n",
       "   0.0042724609375,\n",
       "   0.009185791015625,\n",
       "   0.003509521484375,\n",
       "   0.0140380859375,\n",
       "   0.0029296875,\n",
       "   -0.002227783203125,\n",
       "   0.01568603515625,\n",
       "   0.00018310546875,\n",
       "   0.001800537109375,\n",
       "   -0.01690673828125,\n",
       "   0.0089111328125,\n",
       "   0.07257080078125,\n",
       "   0.04840087890625,\n",
       "   -0.025634765625,\n",
       "   -0.036041259765625,\n",
       "   -0.002777099609375,\n",
       "   0.032135009765625,\n",
       "   0.035980224609375,\n",
       "   -0.001129150390625,\n",
       "   -0.004058837890625,\n",
       "   -0.02880859375,\n",
       "   -0.019775390625,\n",
       "   0.0257568359375,\n",
       "   0.0263671875,\n",
       "   0.02923583984375,\n",
       "   -0.00274658203125,\n",
       "   -0.017120361328125,\n",
       "   0.0162353515625,\n",
       "   0.0247802734375,\n",
       "   0.02197265625,\n",
       "   0.0181884765625,\n",
       "   0.00958251953125,\n",
       "   0.001068115234375,\n",
       "   -0.001678466796875,\n",
       "   0.000885009765625,\n",
       "   -0.00677490234375,\n",
       "   -0.030792236328125,\n",
       "   -0.0341796875,\n",
       "   -0.06427001953125,\n",
       "   -0.05914306640625,\n",
       "   0.00396728515625,\n",
       "   0.0423583984375,\n",
       "   0.03125,\n",
       "   0.010498046875,\n",
       "   -0.02093505859375,\n",
       "   -0.020111083984375,\n",
       "   -0.01336669921875,\n",
       "   -0.011444091796875,\n",
       "   -0.01226806640625,\n",
       "   -0.019622802734375,\n",
       "   -0.011505126953125,\n",
       "   -0.01922607421875,\n",
       "   0.011688232421875,\n",
       "   0.057220458984375,\n",
       "   0.04052734375,\n",
       "   0.014068603515625,\n",
       "   -0.036376953125,\n",
       "   -0.044708251953125,\n",
       "   0.0018310546875,\n",
       "   -0.00518798828125,\n",
       "   0.019775390625,\n",
       "   0.010589599609375,\n",
       "   -0.046905517578125,\n",
       "   -0.03521728515625,\n",
       "   0.0277099609375,\n",
       "   0.07537841796875,\n",
       "   0.06317138671875,\n",
       "   0.020233154296875,\n",
       "   -0.002410888671875,\n",
       "   -0.04144287109375,\n",
       "   -0.034393310546875,\n",
       "   0.036346435546875,\n",
       "   0.042572021484375,\n",
       "   0.05706787109375,\n",
       "   0.01092529296875,\n",
       "   -0.02093505859375,\n",
       "   0.01043701171875,\n",
       "   0.014801025390625,\n",
       "   0.005096435546875,\n",
       "   -0.026641845703125,\n",
       "   -0.054656982421875,\n",
       "   -0.04388427734375,\n",
       "   -0.03717041015625,\n",
       "   -0.02227783203125,\n",
       "   -0.0152587890625,\n",
       "   -0.019683837890625,\n",
       "   -0.03326416015625,\n",
       "   -0.038055419921875,\n",
       "   -0.019927978515625,\n",
       "   -0.016845703125,\n",
       "   -0.02935791015625,\n",
       "   -0.00830078125,\n",
       "   -0.002960205078125,\n",
       "   -0.001739501953125,\n",
       "   0.011749267578125,\n",
       "   0.0107421875,\n",
       "   0.0115966796875,\n",
       "   0.01348876953125,\n",
       "   0.011077880859375,\n",
       "   0.016021728515625,\n",
       "   0.003814697265625,\n",
       "   0.004730224609375,\n",
       "   0.036407470703125,\n",
       "   0.0484619140625,\n",
       "   0.030120849609375,\n",
       "   -0.008209228515625,\n",
       "   0.002166748046875,\n",
       "   0.029998779296875,\n",
       "   0.03326416015625,\n",
       "   0.027618408203125,\n",
       "   -0.00543212890625,\n",
       "   -0.00140380859375,\n",
       "   0.00238037109375,\n",
       "   0.01806640625,\n",
       "   0.026275634765625,\n",
       "   0.029510498046875,\n",
       "   0.0328369140625,\n",
       "   -0.02435302734375,\n",
       "   -0.0909423828125,\n",
       "   -0.057830810546875,\n",
       "   -0.013427734375,\n",
       "   0.05389404296875,\n",
       "   0.063873291015625,\n",
       "   0.00054931640625,\n",
       "   -0.0125732421875,\n",
       "   -0.000823974609375,\n",
       "   -0.008209228515625,\n",
       "   -0.03021240234375,\n",
       "   -0.011749267578125,\n",
       "   0.0377197265625,\n",
       "   0.052764892578125,\n",
       "   0.027374267578125,\n",
       "   -0.031158447265625,\n",
       "   -0.040618896484375,\n",
       "   ...],\n",
       "  'path': '/home/peterr/macocu/task8/transfer_10/5T4HXO1WsDU_2830.21-2850.13.wav',\n",
       "  'sampling_rate': 16000}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec54f52349e4e6eaeb06ba7aaa92c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24501 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390da6ec65de47cba478d721687756bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\n",
    "    \"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(\n",
    "    feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "import torch\n",
    "\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "common_voice_train_mapped = common_voice_train_dataset.map(\n",
    "    prepare_dataset, remove_columns=common_voice_train_dataset.column_names)\n",
    "common_voice_test_mapped = common_voice_test_dataset.map(\n",
    "    prepare_dataset, remove_columns=common_voice_test_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164ea3c9775f4315b66fa093b09f9a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/facebook/wav2vec2-xls-r-300m/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dabc27df63e37bd2a7a221c7774e35f36a280fbdf917cf54cadfc7df8c786f6f.a3e4c3c967d9985881e0ae550a5f6f668f897db5ab2e0802f9b97973b15970e6\n",
      "Model config Wav2Vec2Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 39,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/wav2vec2-xls-r-300m/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/1e6a6507f3b689035cd4b247e2a37c154e27f39143f31357a49b4e38baeccc36.1edb32803799e27ed554eb7dd935f6745b1a0b17b0ea256442fe24db6eb546cd\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-xls-r-300m were not used when initializing Wav2Vec2ForCTC: ['project_q.weight', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_hid.bias', 'project_q.bias', 'quantizer.weight_proj.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1709: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 24243\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 3032\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m5roop\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/5roop/huggingface/runs/pclrmj43\" target=\"_blank\">300_</a></strong> to <a href=\"https://wandb.ai/5roop/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3032' max='3032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3032/3032 13:44:59, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.162286</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.140359</td>\n",
       "      <td>0.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>0.141079</td>\n",
       "      <td>0.882000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to 300_/checkpoint-400\n",
      "Configuration saved in 300_/checkpoint-400/config.json\n",
      "Model weights saved in 300_/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in 300_/checkpoint-400/preprocessor_config.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to 300_/checkpoint-800\n",
      "Configuration saved in 300_/checkpoint-800/config.json\n",
      "Model weights saved in 300_/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in 300_/checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [300_/checkpoint-400] due to args.save_total_limit\n",
      "Saving model checkpoint to 300_/checkpoint-1200\n",
      "Configuration saved in 300_/checkpoint-1200/config.json\n",
      "Model weights saved in 300_/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in 300_/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [300_/checkpoint-800] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to 300_/checkpoint-1600\n",
      "Configuration saved in 300_/checkpoint-1600/config.json\n",
      "Model weights saved in 300_/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in 300_/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [300_/checkpoint-1200] due to args.save_total_limit\n",
      "Saving model checkpoint to 300_/checkpoint-2000\n",
      "Configuration saved in 300_/checkpoint-2000/config.json\n",
      "Model weights saved in 300_/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in 300_/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [300_/checkpoint-1600] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to 300_/checkpoint-2400\n",
      "Configuration saved in 300_/checkpoint-2400/config.json\n",
      "Model weights saved in 300_/checkpoint-2400/pytorch_model.bin\n",
      "Feature extractor saved in 300_/checkpoint-2400/preprocessor_config.json\n",
      "Deleting older checkpoint [300_/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to 300_/checkpoint-2800\n",
      "Configuration saved in 300_/checkpoint-2800/config.json\n",
      "Model weights saved in 300_/checkpoint-2800/pytorch_model.bin\n",
      "Feature extractor saved in 300_/checkpoint-2800/preprocessor_config.json\n",
      "Deleting older checkpoint [300_/checkpoint-2400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3032, training_loss=0.4600355276017202, metrics={'train_runtime': 49525.3215, 'train_samples_per_second': 3.916, 'train_steps_per_second': 0.061, 'total_flos': 9.513123799290988e+19, 'train_loss': 0.4600355276017202, 'epoch': 8.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "repo_name = modelname\n",
    "wer_metric = load_metric(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "max_input_length_in_sec = 20\n",
    "common_voice_train_mapped = common_voice_train_mapped.filter(lambda x: x < max_input_length_in_sec * processor.feature_extractor.sampling_rate, input_columns=[\"input_length\"])\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\",\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")\n",
    "\n",
    "model.freeze_feature_extractor()\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  gradient_accumulation_steps=4,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=8,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=400,\n",
    "  eval_steps=800,\n",
    "  logging_steps=400,\n",
    "  learning_rate=6e-4,\n",
    "  warmup_steps=500,\n",
    "  save_total_limit=1,\n",
    "  push_to_hub=False,\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train_mapped,\n",
    "    eval_dataset=common_voice_test_mapped,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
