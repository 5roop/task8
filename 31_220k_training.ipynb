{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tries to train on the finalised dataset (196GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "cuda.select_device(0)\n",
    "torch.cuda.empty_cache()\n",
    "import os\n",
    "os.system(\"bash ~/clean_tmp_script.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_mapped)=220148\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Audio, load_from_disk\n",
    "train_mapped = load_from_disk(\"train220k_mapped\")\n",
    "troubled_idx = [\n",
    "    170,\n",
    "    474,\n",
    "    778,\n",
    "    1082,\n",
    "    1385,\n",
    "    1688,\n",
    "    1991,\n",
    "    2294,\n",
    "    2597,\n",
    "    2900,\n",
    "    3203,\n",
    "    3506,\n",
    "    3809,\n",
    "    4111,\n",
    "    4413,\n",
    "    4715,\n",
    "    5017,\n",
    "    5319,\n",
    "    5620,\n",
    "    5921,\n",
    "    6222,\n",
    "    6523,\n",
    "    6823,\n",
    "    7123,\n",
    "    7422,\n",
    "    7721,\n",
    "    8020,\n",
    "    8319,\n",
    "    8618,\n",
    "    8917,\n",
    "    9216,\n",
    "    9515,\n",
    "    9814,\n",
    "    10113,\n",
    "    10412,\n",
    "    10711,\n",
    "    11010,\n",
    "    11309,\n",
    "    11608,\n",
    "    11907,\n",
    "    12206,\n",
    "    12505,\n",
    "    12804,\n",
    "    13103,\n",
    "    13401,\n",
    "    13699,\n",
    "    13997,\n",
    "    14295,\n",
    "    14593,\n",
    "    14891,\n",
    "    15189,\n",
    "    15487,\n",
    "    15785,\n",
    "    16083,\n",
    "    16381,\n",
    "    16679,\n",
    "    16977,\n",
    "    17275,\n",
    "    17573,\n",
    "    17871,\n",
    "    18169,\n",
    "    18467,\n",
    "    18765,\n",
    "    19063,\n",
    "    19361,\n",
    "    19659,\n",
    "    19957,\n",
    "    20255,\n",
    "    20553,\n",
    "    20850,\n",
    "    21147,\n",
    "    21444,\n",
    "    21741,\n",
    "    22038,\n",
    "    22335,\n",
    "    22632,\n",
    "    22929,\n",
    "    23226,\n",
    "    23523,\n",
    "    23820,\n",
    "    24117,\n",
    "    24414,\n",
    "    24711,\n",
    "    25008,\n",
    "    25305,\n",
    "    25602,\n",
    "    25899,\n",
    "    26196,\n",
    "    26493,\n",
    "    26790,\n",
    "    27087,\n",
    "    27384,\n",
    "    27681,\n",
    "    27978,\n",
    "    28275,\n",
    "    28572,\n",
    "    28869,\n",
    "    29166,\n",
    "    29463,\n",
    "    29760,\n",
    "    30057,\n",
    "    30354,\n",
    "    30651,\n",
    "    30948,\n",
    "    31245,\n",
    "    31542,\n",
    "    31839,\n",
    "    32136,\n",
    "    32433,\n",
    "    32730,\n",
    "    33027,\n",
    "    33324,\n",
    "    33621,\n",
    "    33918,\n",
    "    34215,\n",
    "    34512,\n",
    "    34809,\n",
    "    35106,\n",
    "    35402,\n",
    "    35698,\n",
    "    35994,\n",
    "    36290,\n",
    "    36586,\n",
    "    36882,\n",
    "    37178,\n",
    "    37474,\n",
    "    37770,\n",
    "    38066,\n",
    "    38362,\n",
    "    38658,\n",
    "    38953,\n",
    "    39248,\n",
    "    39543,\n",
    "    39838,\n",
    "    40133,\n",
    "    40428,\n",
    "    40723,\n",
    "    41018,\n",
    "    41313,\n",
    "    41608,\n",
    "    41903,\n",
    "    42198,\n",
    "    42493,\n",
    "    42788,\n",
    "    43083,\n",
    "    43378,\n",
    "    43673,\n",
    "    43968,\n",
    "    44263,\n",
    "    44558,\n",
    "    44853,\n",
    "    45148,\n",
    "    45443,\n",
    "    45738,\n",
    "    46033,\n",
    "    46328,\n",
    "    46623,\n",
    "    46918,\n",
    "    47213,\n",
    "    47508,\n",
    "    47803,\n",
    "    48098,\n",
    "    48393,\n",
    "    48688,\n",
    "    48983,\n",
    "    49278,\n",
    "    49573,\n",
    "    49868,\n",
    "    50163,\n",
    "    50458,\n",
    "    50753,\n",
    "    51048,\n",
    "    51343,\n",
    "    51638,\n",
    "    51933,\n",
    "    52228,\n",
    "    52523,\n",
    "    52818,\n",
    "    53113,\n",
    "    53408,\n",
    "    53703,\n",
    "    53998,\n",
    "    54293,\n",
    "    54588,\n",
    "    54883,\n",
    "    55178,\n",
    "    55473,\n",
    "    55768,\n",
    "    56063,\n",
    "    56358,\n",
    "    56653,\n",
    "    56948,\n",
    "    57242,\n",
    "    57536,\n",
    "    57830,\n",
    "    58124,\n",
    "    58418,\n",
    "    58712,\n",
    "    59006,\n",
    "    59300,\n",
    "    59594,\n",
    "    59888,\n",
    "    60182,\n",
    "    60476,\n",
    "    60770,\n",
    "    61064,\n",
    "    61358,\n",
    "    61652,\n",
    "    61946,\n",
    "    62240,\n",
    "    62534,\n",
    "    62828,\n",
    "    63122,\n",
    "    63416,\n",
    "    63710,\n",
    "    64004,\n",
    "    64298,\n",
    "    64592,\n",
    "    64885,\n",
    "    65178,\n",
    "    65471,\n",
    "    65764,\n",
    "    66057,\n",
    "    66350,\n",
    "    66643,\n",
    "    66936,\n",
    "    67229,\n",
    "    67522,\n",
    "    67815,\n",
    "    68108,\n",
    "    68401,\n",
    "    68694,\n",
    "    68987,\n",
    "    69280,\n",
    "    69573,\n",
    "    69866,\n",
    "    70159,\n",
    "    70452,\n",
    "    70745,\n",
    "    71037,\n",
    "    71329,\n",
    "    71621,\n",
    "    71913,\n",
    "    72205,\n",
    "    72497,\n",
    "    72789,\n",
    "    73081,\n",
    "    73373,\n",
    "    73665,\n",
    "    73957,\n",
    "    74249,\n",
    "    74541,\n",
    "    74833,\n",
    "    75125,\n",
    "    75417,\n",
    "    75709,\n",
    "    76001,\n",
    "    76293,\n",
    "    76585,\n",
    "    76877,\n",
    "    77169,\n",
    "    77461,\n",
    "    77753,\n",
    "    78045,\n",
    "    78337,\n",
    "    78629,\n",
    "    78921,\n",
    "    79213,\n",
    "    79505,\n",
    "    79797,\n",
    "    80089,\n",
    "    80381,\n",
    "    80673,\n",
    "    80965,\n",
    "    81257,\n",
    "    81549,\n",
    "    81840,\n",
    "    82131,\n",
    "    82422,\n",
    "    82713,\n",
    "    83004,\n",
    "    83295,\n",
    "    83586,\n",
    "    83877,\n",
    "    84168,\n",
    "    84459,\n",
    "    84750,\n",
    "    85041,\n",
    "    85332,\n",
    "    85623,\n",
    "    85914,\n",
    "    86205,\n",
    "    86496,\n",
    "    86787,\n",
    "    87078,\n",
    "    87369,\n",
    "    87660,\n",
    "    87951,\n",
    "    88242,\n",
    "    88533,\n",
    "    88824,\n",
    "    89115,\n",
    "    89406,\n",
    "    89697,\n",
    "    89988,\n",
    "    90279,\n",
    "    90570,\n",
    "    90861,\n",
    "    91152,\n",
    "    91443,\n",
    "    91734,\n",
    "    92025,\n",
    "    92316,\n",
    "    92607,\n",
    "    92898,\n",
    "    93189,\n",
    "    93480,\n",
    "    93771,\n",
    "    94062,\n",
    "    94353,\n",
    "    94644,\n",
    "    94935,\n",
    "    95226,\n",
    "    95516,\n",
    "    95806,\n",
    "    96096,\n",
    "    96386,\n",
    "    96676,\n",
    "    96966,\n",
    "    97255,\n",
    "    97544,\n",
    "    97833,\n",
    "    98122,\n",
    "    98411,\n",
    "    98700,\n",
    "    98989,\n",
    "    99278,\n",
    "    99567,\n",
    "    99856,\n",
    "    100145,\n",
    "    100434,\n",
    "    100723,\n",
    "    101012,\n",
    "    101301,\n",
    "    101590,\n",
    "    101879,\n",
    "    102168,\n",
    "    102457,\n",
    "    102746,\n",
    "    103035,\n",
    "    103324,\n",
    "    103613,\n",
    "    103902,\n",
    "    104191,\n",
    "    104480,\n",
    "    104769,\n",
    "    105058,\n",
    "    105347,\n",
    "    105635,\n",
    "    105923,\n",
    "    106211,\n",
    "    106499,\n",
    "    106787,\n",
    "    107075,\n",
    "    107363,\n",
    "    107651,\n",
    "    107939,\n",
    "    108227,\n",
    "    108514,\n",
    "    108801,\n",
    "    109088,\n",
    "    109375,\n",
    "    109662,\n",
    "    109949,\n",
    "    110236,\n",
    "    110523,\n",
    "    110810,\n",
    "    111097,\n",
    "    111384,\n",
    "    111671,\n",
    "    111958,\n",
    "    112245,\n",
    "    112532,\n",
    "    112819,\n",
    "    113106,\n",
    "    113393,\n",
    "    113680,\n",
    "    113967,\n",
    "    114254,\n",
    "    114541,\n",
    "    114828,\n",
    "    115115,\n",
    "    115402,\n",
    "    115689,\n",
    "    115976,\n",
    "    116263,\n",
    "    116550,\n",
    "    116836,\n",
    "    117122,\n",
    "    117408,\n",
    "    117694,\n",
    "    117980,\n",
    "    118266,\n",
    "    118551,\n",
    "    118836,\n",
    "    119121,\n",
    "    119406,\n",
    "    119691,\n",
    "    119976,\n",
    "    120261,\n",
    "    120546,\n",
    "    120831,\n",
    "    121116,\n",
    "    121401,\n",
    "    121686,\n",
    "    121971,\n",
    "    122256,\n",
    "    122541,\n",
    "    122826,\n",
    "    123111,\n",
    "    123396,\n",
    "    123681,\n",
    "    123966,\n",
    "    124251,\n",
    "    124536,\n",
    "    124821,\n",
    "    125106,\n",
    "    125391,\n",
    "    125676,\n",
    "    125961,\n",
    "    126246,\n",
    "    126531,\n",
    "    126816,\n",
    "    127101,\n",
    "    127386,\n",
    "    127671,\n",
    "    127956,\n",
    "    128241,\n",
    "    128526,\n",
    "    128811,\n",
    "    129096,\n",
    "    129380,\n",
    "    129664,\n",
    "    129948,\n",
    "    130232,\n",
    "    130516,\n",
    "    130800,\n",
    "    131084,\n",
    "    131368,\n",
    "    131652,\n",
    "    131936,\n",
    "    132220,\n",
    "    132504,\n",
    "    132788,\n",
    "    133072,\n",
    "    133356,\n",
    "    133640,\n",
    "    133924,\n",
    "    134208,\n",
    "    134492,\n",
    "    134776,\n",
    "    135060,\n",
    "    135344,\n",
    "    135628,\n",
    "    135912,\n",
    "    136196,\n",
    "    136480,\n",
    "    136764,\n",
    "    137048,\n",
    "    137332,\n",
    "    137616,\n",
    "    137900,\n",
    "    138184,\n",
    "    138468,\n",
    "    138752,\n",
    "    139036,\n",
    "    139320,\n",
    "    139604,\n",
    "    139888,\n",
    "    140172,\n",
    "    140456,\n",
    "    140740,\n",
    "    141024,\n",
    "    141308,\n",
    "    141592,\n",
    "    141876,\n",
    "    142160,\n",
    "    142444,\n",
    "    142728,\n",
    "    143012,\n",
    "    143296,\n",
    "    143579,\n",
    "    143862,\n",
    "    144145,\n",
    "    144428,\n",
    "    144711,\n",
    "    144994,\n",
    "    145277,\n",
    "    145560,\n",
    "    145843,\n",
    "    146126,\n",
    "    146409,\n",
    "    146692,\n",
    "    146975,\n",
    "    147258,\n",
    "    147541,\n",
    "    147824,\n",
    "    148107,\n",
    "    148390,\n",
    "    148673,\n",
    "    148956,\n",
    "    149239,\n",
    "    149522,\n",
    "    149805,\n",
    "    150088,\n",
    "    150371,\n",
    "    150654,\n",
    "    150937,\n",
    "    151220,\n",
    "    151502,\n",
    "    151784,\n",
    "    152066,\n",
    "    152348,\n",
    "    152630,\n",
    "    152912,\n",
    "    153194,\n",
    "    153476,\n",
    "    153758,\n",
    "    154040,\n",
    "    154322,\n",
    "    154604,\n",
    "    154886,\n",
    "    155168,\n",
    "    155450,\n",
    "    155732,\n",
    "    156014,\n",
    "    156296,\n",
    "    156578,\n",
    "    156860,\n",
    "    157142,\n",
    "    157424,\n",
    "    157706,\n",
    "    157988,\n",
    "    158270,\n",
    "    158552,\n",
    "    158834,\n",
    "    159116,\n",
    "    159398,\n",
    "    159680,\n",
    "    159962,\n",
    "    160244,\n",
    "    160526,\n",
    "    160808,\n",
    "    161090,\n",
    "    161372,\n",
    "    161654,\n",
    "    161936,\n",
    "    162218,\n",
    "    162500,\n",
    "    162782,\n",
    "    163064,\n",
    "    163346,\n",
    "    163628,\n",
    "    163910,\n",
    "    164192,\n",
    "    164474,\n",
    "    164756,\n",
    "    165038,\n",
    "    165320,\n",
    "    165602,\n",
    "    165884,\n",
    "    166166,\n",
    "    166448,\n",
    "    166730,\n",
    "    167012,\n",
    "    167294,\n",
    "    167576,\n",
    "    167858,\n",
    "    168140,\n",
    "    168422,\n",
    "    168704,\n",
    "    168986,\n",
    "    169268,\n",
    "    169550,\n",
    "    169832,\n",
    "    170114,\n",
    "    170396,\n",
    "    170678,\n",
    "    170960,\n",
    "    171242,\n",
    "    171524,\n",
    "    171806,\n",
    "    172088,\n",
    "    172370,\n",
    "    172652,\n",
    "    172934,\n",
    "    173216,\n",
    "    173498,\n",
    "    173780,\n",
    "    174062,\n",
    "    174344,\n",
    "    174626,\n",
    "    174908,\n",
    "    175190,\n",
    "    175472,\n",
    "    175754,\n",
    "    176036,\n",
    "    176318,\n",
    "    176600,\n",
    "    176882,\n",
    "    177164,\n",
    "    177446,\n",
    "    177728,\n",
    "    178009,\n",
    "    178290,\n",
    "    178571,\n",
    "    178852,\n",
    "    179133,\n",
    "    179414,\n",
    "    179695,\n",
    "    179976,\n",
    "    180257,\n",
    "    180538,\n",
    "    180819,\n",
    "    181100,\n",
    "    181381,\n",
    "    181662,\n",
    "    181943,\n",
    "    182224,\n",
    "    182505,\n",
    "    182786,\n",
    "    183067,\n",
    "    183348,\n",
    "    183629,\n",
    "    183910,\n",
    "    184191,\n",
    "    184472,\n",
    "    184753,\n",
    "    185034,\n",
    "    185315,\n",
    "    185596,\n",
    "    185877,\n",
    "    186158,\n",
    "    186439,\n",
    "    186720,\n",
    "    187001,\n",
    "    187282,\n",
    "    187563,\n",
    "    187844,\n",
    "    188125,\n",
    "    188406,\n",
    "    188687,\n",
    "    188968,\n",
    "    189249,\n",
    "    189530,\n",
    "    189811,\n",
    "    190092,\n",
    "    190373,\n",
    "    190654,\n",
    "    190935,\n",
    "    191216,\n",
    "    191497,\n",
    "    191777,\n",
    "    192057,\n",
    "    192337,\n",
    "    192617,\n",
    "    192897,\n",
    "    193177,\n",
    "    193457,\n",
    "    193737,\n",
    "    194017,\n",
    "    194297,\n",
    "    194577,\n",
    "    194857,\n",
    "    195137,\n",
    "    195417,\n",
    "    195697,\n",
    "    195977,\n",
    "    196257,\n",
    "    196537,\n",
    "    196817,\n",
    "    197097,\n",
    "    197377,\n",
    "    197657,\n",
    "    197937,\n",
    "    198217,\n",
    "    198497,\n",
    "    198777,\n",
    "    199057,\n",
    "    199337,\n",
    "    199617,\n",
    "    199897,\n",
    "    200177,\n",
    "    200457,\n",
    "    200737,\n",
    "    201017,\n",
    "    201297,\n",
    "    201577,\n",
    "    201857,\n",
    "    202137,\n",
    "    202417,\n",
    "    202697,\n",
    "    202977,\n",
    "    203257,\n",
    "    203537,\n",
    "    203817,\n",
    "    204097,\n",
    "    204377,\n",
    "    204657,\n",
    "    204937,\n",
    "    205217,\n",
    "    205497,\n",
    "    205777,\n",
    "    206057,\n",
    "    206337,\n",
    "    206617,\n",
    "    206897,\n",
    "    207177,\n",
    "    207457,\n",
    "    207737,\n",
    "    208017,\n",
    "    208297,\n",
    "    208577,\n",
    "    208857,\n",
    "    209137,\n",
    "    209417,\n",
    "    209697,\n",
    "    209977,\n",
    "    210257,\n",
    "    210537,\n",
    "    210817,\n",
    "    211097,\n",
    "    211377,\n",
    "    211657,\n",
    "    211937,\n",
    "    212217,\n",
    "    212497,\n",
    "    212777,\n",
    "    213057,\n",
    "    213337,\n",
    "    213617,\n",
    "    213897,\n",
    "    214177,\n",
    "    214457,\n",
    "    214737,\n",
    "    215017,\n",
    "    215296,\n",
    "    215575,\n",
    "    215854,\n",
    "    216133,\n",
    "    216412,\n",
    "    216691,\n",
    "    216970,\n",
    "    217249,\n",
    "    217528,\n",
    "    217807,\n",
    "    218086,\n",
    "    218365,\n",
    "    218644,\n",
    "    218923,\n",
    "    219202,\n",
    "    219481,\n",
    "    219760,\n",
    "    220039]\n",
    "\n",
    "test_mapped= load_from_disk(\"test_mapped\")\n",
    "print(f\"{len(train_mapped)=}\")\n",
    "\n",
    "# for i in range(len(train_mapped)):\n",
    "#     if len(train_mapped[i][\"input_values\"]) == 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Use old or new vocab?\n",
    "os.system(\"cp vocab_300_with_numbers.json vocab.json\")\n",
    "\n",
    "\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\n",
    "    \"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\" \")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(\n",
    "    feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at train220k_mapped/cache-b642bb1a81d8c9b0.arrow\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-xls-r-300m were not used when initializing Wav2Vec2ForCTC: ['project_hid.weight', 'quantizer.weight_proj.weight', 'project_q.weight', 'project_q.bias', 'project_hid.bias', 'quantizer.codevectors', 'quantizer.weight_proj.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1709: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 215556\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 6736\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m5roop\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "modelname = \"31_normalised_220k\"\n",
    "repo_name = modelname\n",
    "wer_metric = load_metric(\"wer\")\n",
    "cer_metric = load_metric(\"cer\")\n",
    "def compute_metrics(pred):\n",
    "    import numpy as np\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "max_input_length_in_sec = 20\n",
    "train_mapped = train_mapped.filter(lambda x: x < max_input_length_in_sec * processor.feature_extractor.sampling_rate, input_columns=[\"input_length\"])\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\",\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")\n",
    "\n",
    "model.freeze_feature_extractor()\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  gradient_accumulation_steps=4,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  num_train_epochs=2,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "#   save_steps=1187,\n",
    "#   eval_steps=1187//4,\n",
    "  logging_steps=400,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=500,\n",
    "  save_total_limit=5,\n",
    "  push_to_hub=False,\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_mapped,\n",
    "    eval_dataset=test_mapped,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_idx = []\n",
    "for i, item in enumerate(train_mapped):\n",
    "    input_values = item[\"input_values\"]\n",
    "    labels = item[\"labels\"]\n",
    "    if len(input_values) == 0 or len(labels) == 0:\n",
    "        print(f\"{i=}, {len(input_values)=}, {len(labels)=}\")\n",
    "        problematic_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mapped[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_mapped[474][\"labels\"] == train_mapped[1688][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mapped[0][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
